---
layout: page
title: Point-Cloud-Based Detection and Tracking
permalink: /research/detection-and-tracking.html
---

### Contents:

* [Introduction](#id1)
* [Demo Videos](#id2)
* [Researchers](#id3)
* [Acknowledgement](#id5)

<!-- Title your work here -->

## Point-Cloud-Based Detection and Tracking for Autonomous Driving


<!-- Add your own introduction here -->

### <a name="id1"></a>Introduction

Detection and tracking of road participants such as vehicles, pedestrians and cyclists are the prerequisites for precise intention recognition and motion prediction, as well as safe decision-making and planning. Point cloud from Lidars has been proved to be a reliable and precise source for detection and tracking by providing accurate distance and shape information.

In this independent project, clustering algorithms and L-shape fitting methods are developed to detect road participants via point-cloud from Lidars. Also, we improve data association and state estimation algorithms for tracking of the obstacles. The demo video illustrates the results of our algorithms for vehicle detection and tracking at an intersection with vector map via point cloud from a 64-layer Velodyne Lidar.



### <a name="id2"></a>Demo Videos

<div style="position: relative; width: 100%; height: 0; padding-bottom: 56.3%;">
<iframe style = "position: absolute; width: 100%; height: 100%; left: 0; top: 0;"
  src="https://youtube.com/embed//6KXJHwFekE0?rel=0" frameborder="0" controls="controls" preload="auto" allowfullscreen></iframe>
</div>

<div class="image-caption">Vehicle detection and tracking at an intersection based on point cloud from a 64-layer Lidar</div>



### <a name="id3"></a>Researchers

| Taohan Wang | Graduate Student | [Email Link](mailto:taohanwang@berkeley.edu)|

| Wei Zhan | Graduate Student | [Email Link](mailto:wzhan@berkeley.edu) | 



<!-- If you have any sponsors, you can just list them here -->

### <a name="id5"></a>Acknowledgement

* We thank Berkeley DeepDrive for the support of data collection platform. We also highly appreciate the help from Dr. Yi-Ta Chuang.